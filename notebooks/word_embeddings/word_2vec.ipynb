{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings\n",
    "\n",
    "1. average the vector for each single word in ImageNet list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#As model we use a pretrained model from FastText:\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# https://fasttext.cc/docs/en/pretrained-vectors.html\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mKeyedVectors\u001b[39m.\u001b[39;49mload_word2vec_format(\u001b[39m'\u001b[39;49m\u001b[39m./wiki.en.vec\u001b[39;49m\u001b[39m'\u001b[39;49m, binary\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\David\\anaconda3\\envs\\ise\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m   1673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1674\u001b[0m         \u001b[39mcls\u001b[39m, fname, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1675\u001b[0m         limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, datatype\u001b[39m=\u001b[39mREAL, no_header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m     ):\n\u001b[0;32m   1677\u001b[0m     \u001b[39m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[0;32m   1679\u001b[0m \u001b[39m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[0;32m   1720\u001b[0m         \u001b[39mcls\u001b[39;49m, fname, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, encoding\u001b[39m=\u001b[39;49mencoding, unicode_errors\u001b[39m=\u001b[39;49municode_errors,\n\u001b[0;32m   1721\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit, datatype\u001b[39m=\u001b[39;49mdatatype, no_header\u001b[39m=\u001b[39;49mno_header,\n\u001b[0;32m   1722\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\David\\anaconda3\\envs\\ise\\lib\\site-packages\\gensim\\models\\keyedvectors.py:2059\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2057\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2058\u001b[0m     header \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mto_unicode(fin\u001b[39m.\u001b[39mreadline(), encoding\u001b[39m=\u001b[39mencoding)\n\u001b[1;32m-> 2059\u001b[0m     vocab_size, vector_size \u001b[39m=\u001b[39m [\u001b[39mint\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m header\u001b[39m.\u001b[39msplit()]  \u001b[39m# throws for invalid file format\u001b[39;00m\n\u001b[0;32m   2060\u001b[0m \u001b[39mif\u001b[39;00m limit:\n\u001b[0;32m   2061\u001b[0m     vocab_size \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(vocab_size, limit)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "#As model we use a pretrained model from FastText:\n",
    "# https://fasttext.cc/docs/en/pretrained-vectors.html\n",
    "# This model has 6gb of data DISCUSSION: Should we use it?\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./wiki.en.vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec_model’ parameter is the loaded word2vec model\n",
    "#‘words’ parameter is a tokens list (strings) of the sentence / paragraph / document that you want to calculate\n",
    "def get_mean_vector(word2vec_model, words):\n",
    "    # remove out-of-vocabulary words\n",
    "    words = [word for word in words if word in word2vec_model.vocab]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean(word2vec_model[words], axis=0)\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgnet = pd.read_csv(\"../files/ImageNet.csv\")\n",
    "class_list = df_imgnet[\"Class Name\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test \u001b[39m=\u001b[39m get_mean_vector(model, class_list)\n",
      "Cell \u001b[1;32mIn [16], line 5\u001b[0m, in \u001b[0;36mget_mean_vector\u001b[1;34m(word2vec_model, words)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_mean_vector\u001b[39m(word2vec_model, words):\n\u001b[0;32m      4\u001b[0m     \u001b[39m# remove out-of-vocabulary words\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     words \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m word2vec_model\u001b[39m.\u001b[39mvocab]\n\u001b[0;32m      6\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(words) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m      7\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(word2vec_model[words], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn [16], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_mean_vector\u001b[39m(word2vec_model, words):\n\u001b[0;32m      4\u001b[0m     \u001b[39m# remove out-of-vocabulary words\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     words \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m word2vec_model\u001b[39m.\u001b[39;49mvocab]\n\u001b[0;32m      6\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(words) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m      7\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(word2vec_model[words], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "test = get_mean_vector(model, class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "corpus = api.load('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in corpus:\n",
    "    vec = get_mean_vector(model, doc.words)\n",
    "    if len(vec) > 0:\n",
    "      # do somthing with the vector ${vec}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. take zero vectors for words can not be vectorized. for example for hammerhead shark, hammerhead could be zero, but still /2 because two words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Approach:\n",
    "Take each item in the ImageNet list, prepare to each dbpedia Class and take the highest score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Approach: Average the vector of the items in imageNet class.\n",
    "In the End we gotTwo sets of results. \n",
    "Write a function for top three matchings of dbpedia_class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d19f4e4876cde0e36c0fc2e1d01865a16d321819d50c212123773084521aaad0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
