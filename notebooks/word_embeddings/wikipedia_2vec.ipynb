{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install wikipedia2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wikipedia2vec.Wikipedia2Vec.load(\"../enwiki_20180420_win10_300d.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_cosine_similarity(feature_vec_1, feature_vec_2):\n",
    "    return cosine_similarity(feature_vec_1.reshape(1, -1), feature_vec_2.reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_net = pd.read_csv(\"../files/ImageNet.csv\")\n",
    "img_net_list = img_net[\"Class Name\"].tolist()\n",
    "\n",
    "dbpedia = pd.read_csv(\"../files/dbpedia_classes.csv\")\n",
    "db_list = dbpedia[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(index =db_list ,columns =img_net_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vec = np.zeros(300, dtype=int)\n",
    "def get_mean_vector_new(word2vec_model, words):\n",
    "    # remove out-of-vocabulary words\n",
    "    #words = [word for word in words if word in word2vec_model.key_to_index]\n",
    "    process_words = []\n",
    "    for el in words:\n",
    "        available = False\n",
    "        try:\n",
    "            model.get_word_vector(el)\n",
    "            available = True\n",
    "        except:\n",
    "            pass\n",
    "        if available:\n",
    "            process_words.append(model.get_word_vector(el))\n",
    "        else:\n",
    "            multi = el.split(\",\")\n",
    "            #new_val = []\n",
    "            arr = np.ndarray(shape=(len(multi),300))\n",
    "            i = 0\n",
    "            for elem in multi:\n",
    "                elem = elem.strip()\n",
    "                mult_words = elem.split(\" \")\n",
    "                elem_av = False\n",
    "                try:\n",
    "                    model.get_word_vector(elem)\n",
    "                    elem_av = True\n",
    "                except:\n",
    "                    pass\n",
    "                if len(mult_words) > 1:\n",
    "                    mult_vec = np.ndarray(shape=(len(mult_words),300))\n",
    "                    j = 0\n",
    "                    for e in mult_words:\n",
    "                        av = False\n",
    "                        try:\n",
    "                            model.get_word_vector(e)\n",
    "                            av = True\n",
    "                        except:\n",
    "                            pass\n",
    "                        if av:\n",
    "                            mult_vec[j] = model.get_word_vector(e)\n",
    "                            j += 1\n",
    "                        else:\n",
    "                            mult_vec[j] = zero_vec\n",
    "                            j += 1\n",
    "                    arr[i] = np.mean(mult_vec, axis = 0)\n",
    "                    i += 1\n",
    "                elif elem_av:\n",
    "                    #new_val.append(model[elem])\n",
    "                    arr[i] = model.get_word_vector(elem)\n",
    "                    i += 1\n",
    "                else:\n",
    "                    #new_val.append(zero_vec)\n",
    "                    arr[i] = zero_vec\n",
    "                    i += 1\n",
    "            process_words.append(np.mean(arr, axis=0))#geht das so mit dem np.mean PROBLEM we receive one value but should receive a matrix                \n",
    "\n",
    "    return process_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "el_img_vec = get_mean_vector_new(model, img_net_list) # is size 1000 \n",
    "#el_img_vec[48][57] = 0.0\n",
    "zero_vec = np.zeros(300, dtype=int)\n",
    "\n",
    "for i in range(0,len(el_img_vec)):\n",
    "    for el_db in db_list:\n",
    "        a = db_list.index(el_db) \n",
    "        multi_db = el_db.split(\" \")\n",
    "        if len(multi_db) > 1:\n",
    "            m_db = np.ndarray(shape=(len(multi_db),300))\n",
    "            x = 0\n",
    "            for e in multi_db:\n",
    "                e_available = False\n",
    "                try:\n",
    "                    model.get_word_vector(e)\n",
    "                    e_available = True \n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                if e_available:\n",
    "                    m_db[x] = model.get_word_vector(e)\n",
    "                    x += 1\n",
    "                else:\n",
    "                    m_db[x] = zero_vec\n",
    "                    x += 1\n",
    "            dbpedia = np.mean(m_db, axis=0)\n",
    "            \n",
    "            sim = get_cosine_similarity(el_img_vec[i],dbpedia)\n",
    "            dataframe.iat[a,i] = sim            \n",
    "        else:\n",
    "            try:\n",
    "                sim = get_cosine_similarity(el_img_vec[i],model[el_db])\n",
    "                dataframe.iat[a,i] = sim                \n",
    "            except:\n",
    "                dataframe.iat[a,i] = \"Not Present\"  # this happens if the dbpedia word is not present in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataframe\n",
    "a = dataframe.replace(\"Not Present\", np.nan)\n",
    "a =a.replace(0.0, np.nan)\n",
    "\n",
    "#Now we remove all columns where we have only NaN values\n",
    "nan = a.dropna(axis=1, how=\"all\")\n",
    "highest_values_nan = nan.apply(lambda s: s.abs().nlargest(5).index.tolist(), axis=0) #change nlargest for top3 or top5\n",
    "highest_values_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  \n",
    "\n",
    "filepath = Path('../files/word_embeddings/wiki_2_vec_top5_mean.csv')  \n",
    "\n",
    "filepath.parent.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "highest_values_nan.to_csv(filepath, index=False)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max-Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives in a list of arrays and returns the mean\n",
    "def preprocess_array(mult_list, split_criterion:str):\n",
    "    output_vec = []\n",
    "    zero_vec = np.zeros(300, dtype=float)\n",
    "    for el in mult_list.split(split_criterion):\n",
    "        prep_el = False\n",
    "        try:\n",
    "            model.get_word_vector(el)\n",
    "            prep_el = True \n",
    "        except:\n",
    "            pass\n",
    "        if prep_el:\n",
    "            output_vec.append(model.get_word_vector(el))\n",
    "        else:\n",
    "            output_vec.append(zero_vec)\n",
    "    return np.mean(output_vec, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_max = pd.DataFrame(index =db_list ,columns =img_net_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#el_img_vec = get_mean_vector_new(model, class_list) # is size 1000 \n",
    "\n",
    "zero_vec = np.zeros(300, dtype=int)\n",
    "\n",
    "for el_img in img_net_list:\n",
    "    for el_db in db_list:\n",
    "        a = db_list.index(el_db)\n",
    "        i = img_net_list.index(el_img) \n",
    "        multi_db = el_db.split(\" \")\n",
    "        if len(multi_db) > 1:\n",
    "            m_db = np.ndarray(shape=(len(multi_db),300))\n",
    "            x = 0\n",
    "            for e in multi_db:\n",
    "                e_ava_max = False\n",
    "                try:\n",
    "                    model.get_word_vector(e)\n",
    "                    e_ava_max = True\n",
    "                except:\n",
    "                    pass\n",
    "                if e_ava_max:\n",
    "                    m_db[x] = model.get_word_vector(e)\n",
    "                    x += 1\n",
    "                else:\n",
    "                    m_db[x] = zero_vec\n",
    "                    x += 1\n",
    "            dbpedia = np.mean(m_db, axis=0)\n",
    "            sim = -1\n",
    "            for el in el_img.split(\",\"):\n",
    "                elxx = preprocess_array(model, el, \" \")\n",
    "            \n",
    "                val = get_cosine_similarity(elxx,dbpedia)\n",
    "                if val > sim:\n",
    "                    sim = val\n",
    "            dataframe_max.iat[a,i] = sim            \n",
    "        else:\n",
    "            try:\n",
    "                sim = -1\n",
    "                for elem in el_img.split(\",\"):\n",
    "\n",
    "                    elem = preprocess_array(model, elem, \" \")\n",
    "            \n",
    "                    val = get_cosine_similarity(elem,model[el_db])\n",
    "                    if val > sim:\n",
    "                        sim = val\n",
    "                #sim = get_cosine_similarity(el_img_vec[i],model[el_db])\n",
    "                dataframe_max.iat[a,i] = sim                \n",
    "            except:\n",
    "                dataframe_max.iat[a,i] = \"Not Present\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataframe_max\n",
    "a = dataframe_max.replace(\"Not Present\", np.nan)\n",
    "a =a.replace(0.0, np.nan)\n",
    "\n",
    "#Now we remove all columns where we have only NaN values\n",
    "nan = a.dropna(axis=1, how=\"all\")\n",
    "highest_values_nan_max = nan.apply(lambda s: s.abs().nlargest(5).index.tolist(), axis=0)\n",
    "highest_values_nan_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  \n",
    "\n",
    "filepath = Path('../files/word_embeddings/wiki_2_vec_top5_max.csv')  \n",
    "\n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "\n",
    "highest_values_nan_max.to_csv(filepath, index=False)  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
